---
lane: agent-skill
pack: prd-guide
built_at: 2026-01-21T12:00:00.000Z
git_commit: 8ae4c307af6368eaf51ed85cd351c86d04a87cae
canon_version: "0.8.0"
sources:
  - canon/README.md
  - odd/README.md
  - odd/manifesto.md
  - odd/cognitive-partitioning.md
  - odd/appendices/README.md
  - odd/decisions/README.md
  - canon/odd/appendices/tool-specialization.md
  - canon/constraints.md
  - canon/decision-rules.md
  - canon/definition-of-done.md
  - canon/self-audit.md
  - docs/PRD/PRD_TEMPLATE.md
  - products/agent-skill/v1.2.4/attempts/attempt-001/INSTRUCTIONS.md
source_hashes:
  canon/README.md: 98ff95bee152af56eec56c6b451e314fe1f144da260c27f3d49847924970620b
  odd/README.md: cdbdff24383a85dacf361099b60a947747afbeb56b03e7636130c0b97daa4a50
  odd/manifesto.md: 8a815ada6af26763e0cdd79eeb21c76eed1fbc7b1cd13068d338535eafa675da
  odd/cognitive-partitioning.md: 92debb039570f9d7225359a4ee918902cd767dc049b1b068791fad05725947d4
  odd/appendices/README.md: 542606e743385b985682891a0f13ca1b463c6f72a0021f620e7bc74dfd12a516
  odd/decisions/README.md: 1f03da50ea51115715ce36ad0beb7d71d06d5df3b3a347dc1c5e1873cc0fb278
  canon/odd/appendices/tool-specialization.md: 2cde355160a8847b7c196f57c9fab896d8e2bdc36bd1ff34abbcfba019080a59
  canon/constraints.md: 5e1846a12abcc12f148775ea31c5aef65ce2151385447c87730b54124de60bca
  canon/decision-rules.md: 4e9b0f9db33474d088d617e665c4ca01cefdeb22bc3bb05429217eeea3a7b481
  canon/definition-of-done.md: afc9f8c5bce0d5a1475110cd7efb3efd3b7050d3c1ff52b77f589fd2125dde35
  canon/self-audit.md: 37e031cef314d6f87dad5dc3682feb5cd808325dac3dc903e0926eca8e1e25c3
  docs/PRD/PRD_TEMPLATE.md: 9ff8b63a6edd0314c4ea884cc3915f6d448949a7d415a3010280aff122cf1afb
  products/agent-skill/v1.2.4/attempts/attempt-001/INSTRUCTIONS.md: 0fc8d637a4021c7c579ed0f936dedffa8e2b96787d4d762b38c3e79b137a8dfa
---


---

## Source: `canon/README.md`

---
uri: klappy://canon
title: "Canon"
audience: canon
exposure: nav
tier: 1
voice: neutral
stability: stable
tags: ["canon", "index", "orientation"]
---

# üß≠ Canon

Curated documents that capture how decisions are made, what assumptions are held, how work is verified, and how rigor changes as projects mature.

The Canon exists so that reasoning does not have to be repeated.

---

## üìÅ Contents

### Core Documents

| File | Title | Summary | Answers |
|------|-------|---------|---------|
| `constraints.md` | Constraints | Baseline assumptions and non-negotiables that shape every decision. | What must be true for this work to make sense? |
| `decision-rules.md` | Decision Rules | Default heuristics used when multiple valid options exist. | How do choices tend to be made? |
| `definition-of-done.md` | Definition of Done | What qualifies as completed work and what evidence is required. | When can work honestly be called done? |
| `self-audit.md` | Self-Audit Checklist | Review checklist before declaring completion. | What should be reviewed before claiming success? |
| `visual-proof.md` | Visual Proof Standards | What qualifies as acceptable visual evidence. | What does "prove it visually" mean? |
| `completion-report-template.md` | Completion Report Template | Standard format for reporting completed work. | How should completion be communicated? |
| `CHANGELOG.md` | Canon Changelog | Version history of canon changes. | What changed and when? |

### Subfolders

| Folder | Purpose |
|--------|---------|
| `meta/` | Metadata and pack configuration. |
| `_compiled/` | Compiled outputs (derived, wipeable). |
| `odd/appendices/` | ODD-derived patterns and invariants. |

### ODD Appendices (Patterns)

| File | Title | Summary |
|------|-------|---------|
| `odd/appendices/tool-specialization.md` | Tool Specialization | Preserve reliability as tool availability increases by isolating tool usage behind narrowly scoped reasoning units. |

---

## üöÄ Start Here

1. **`constraints.md`** ‚Äî What must be true for this work to make sense?
2. **`definition-of-done.md`** ‚Äî When can work honestly be called done?
3. **`/odd/manifesto.md`** ‚Äî Why this approach exists.

These three documents anchor everything else.

---

## üìñ Precedence & Interpretation

A useful mental model for reading:

1. ODD Manifesto provides philosophical grounding
2. Maturity Model explains when rigor increases
3. Constraints shape the solution space
4. Decision Rules guide choices
5. Evidence Policies define completion

If documents appear to conflict, maturity context and explicit tradeoffs usually explain why.

---

## üß† What the Canon Is (and Is Not)

**The Canon Is:**
- A shared reference
- A source of assumptions and defaults
- A way to encode thinking without enforcing execution

**The Canon Is Not:**
- A workflow
- A command system
- A task list
- A replacement for judgment

Nothing in the Canon executes by itself.

---

## üîí Public vs Internal Boundary

- `/odd/README.md` ‚Üí public-facing ODD (shareable, human-friendly)
- `/canon/**` ‚Üí internal reference (governance artifacts, precise language)

Public documents explain intent. Canon documents preserve precision.

---

## üìã Meta Rules

Structural conventions for keeping the Canon coherent over time. These are not workflows or enforcement steps.

**1. Single Inventory Source**
If an inventory of Canon resources exists, there should be one authoritative source (e.g., a manifest). Other indexes should be derived or optional.

**2. Stable Names Beat Clever Names**
Prefer stable file and URI naming over clever branding. Rename rarely.

**3. Audience Separation Matters**
"Public" explains and invites. "Canon" defines and stabilizes.

**4. Voice Is Labeled, Not Transformed**
First-person documents may be consumed as-is or translated by clients. The Canon itself does not require a specific rendering voice.

**5. Multi-Lane PRD Architecture**
PRDs are organized into independent product lanes. Each lane has its own active PRD, attempts, and lifecycle. Lanes share canon, not lifecycle. See `/docs/appendices/product-lanes.md` for the full model.

---

## üîÑ Stability & Change Philosophy

Not all Canon documents are equally stable.

Some are intended to remain largely fixed. Others are expected to evolve through use.

Change is allowed, but should be:
- Intentional
- Versioned (at least informally)
- Documented somewhere discoverable

---

## ‚ö†Ô∏è Confidence & Drift Risk

This section expresses current confidence that the Canon and surrounding architecture align with the core pillars: KISS, DRY, Consistency, Maintainability, Antifragile, Scalable, Prompt-over-Code.

These are not guarantees. They are a snapshot of perceived risk.

**Confidence scale:**
- 0.9+ ‚Äî robust
- 0.7‚Äì0.85 ‚Äî strong, but watch for drift
- 0.5‚Äì0.7 ‚Äî plausible, fragile under misuse
- <0.5 ‚Äî likely misaligned unless corrected

**Current scores (v0.1 snapshot):**

| Pillar | Score | Notes |
|--------|-------|-------|
| Prompt-over-Code | 0.80 | Strong fit. Risk: schema sprawl or client-specific conventions. |
| KISS | 0.75 | Minimal primitives. Risk: meta-layer creep. |
| DRY (with isolation) | 0.70 | Canon centralizes principles. Risk: duplicate indices drifting. |
| Consistency | 0.65 | URI/metadata structure supports it. Risk: naming drift. |
| Maintainability | 0.70 | Stable worldview vs evolving templates. Risk: manual updates out of sync. |
| Antifragile | 0.60 | Recoverable if served statically. Risk: hidden single points of failure. |
| Scalable | 0.70 | Schema supports growth. Risk: large manifests becoming brittle. |

---

## üö´ What Is Intentionally Undefined

The Canon deliberately does not define:
- Specific tools
- Specific agents
- Specific workflows
- Specific automation loops

These are left open to evolve without rewriting foundational thinking.

---

## üì¶ For Pack Compilation

When building a guide pack, include:
- This README for orientation
- Specific documents needed for the pack's purpose
- Subfolder READMEs for scannable summaries without including all files

See `/docs/appendices/compilation.md` for the compilation model.

---

## üîó See Also

- [ODD (Universal Principles)](/odd/README.md) ‚Äî Timeless methodology that Canon derives from
- [Implementation Docs](/docs/README.md) ‚Äî How klappy.dev implements Canon
- [Three-Tier Hierarchy](/odd/decisions/D0001-three-tier-conceptual-hierarchy.md) ‚Äî Why ODD, Canon, and Docs are separate

---

## ‚úÖ Status

- Canon Index v0.1 complete
- Orientation-only
- Includes confidence and drift snapshot

This Canon v0.1 is considered stable for initial builds. Revisions should be additive unless a documented failure requires change.


---

## Source: `odd/README.md`

---
uri: klappy://public/odd
title: "ODD Manifesto ‚Äî Public"
audience: public
exposure: nav
tier: 0
voice: neutral
stability: semi_stable
tags: ["odd", "public", "overview"]
assets: {"practice_video":"/assets/odd/odd-in-practice.mp4","misconception_image":"/assets/odd/odd-is-not-a-framework.png","deep_dive_audio":"/assets/odd/why-evidence-beats-confidence.m4a"}
---

# üß† Outcomes-Driven Development (ODD)

Outcomes-Driven Development (ODD) is an approach to building software that prioritizes real-world results over artifacts.

In an environment where AI can generate code, interfaces, and entire applications quickly, the limiting factor is no longer production speed‚Äîit is clarity of intent, quality of verification, and the ability to choose among many possible outcomes.

ODD exists to make those constraints explicit.

---

## The Core Idea

Traditional software development often optimizes for outputs:

- lines of code
- shipped features
- completed tickets

ODD shifts the focus to outcomes:

- Does this solve the real problem?
- Can it be demonstrated, not just explained?
- Will it hold up as conditions change?

Code is still written. Tools still matter. But they are means, not ends.

---

## Why ODD Now

AI changes the economics of software creation.

When generation becomes cheap:

- variation explodes
- artifacts become disposable
- maintenance becomes the real cost

ODD responds by:

- treating code as ephemeral
- emphasizing verification over explanation
- encouraging curation over accumulation

The goal is not to generate _more_ software, but to ship _better_ outcomes with less long-term drag.

---

## Core Principles

ODD is guided by a small set of principles that recur across projects:

- **Prompt over Code**
  Natural language intent guides generation; code is an output, not the source of truth.

- **Keep It Simple (KISS)**
  Prefer the simplest solution that works and can be explained clearly.

- **Don't Repeat Yourself (DRY), with Isolation**
  Reuse ideas and components where it helps, but avoid brittle global coupling.

- **Consistency**
  Similar problems should feel similar to users and maintainers.

- **Maintainability**
  Optimize for low-effort upkeep and clear handoff, not cleverness.

- **Antifragility**
  Systems should learn from stress and failure, not just survive them.

- **Scalability**
  Growth should increase capability without exploding complexity or cost.

These principles are lenses, not rules. Their application changes as projects mature.

---

## Evidence Over Explanation

ODD places a strong emphasis on evidence.

In practice, this means:

- showing working systems
- favoring visual or experiential proof
- treating explanations as hypotheses until verified

This is especially important in AI-assisted workflows, where fluent explanations are easy to produce but easy to trust incorrectly.

---

## Project Maturity Matters

ODD does not apply the same rigor at every stage.

- **Exploration (PoC)** ‚Äî bias toward learning and speed
- **Validation (Pilot)** ‚Äî bias toward proof and repeatability
- **Commitment (Production)** ‚Äî bias toward trust, durability, and handoff

Rigor increases with maturity. Governance tightens gradually. There are no sharp lines.

---

## What ODD Is Not

ODD is not:

- a framework to install
- a fixed workflow
- a claim that outcomes can be fully predicted

It does not replace judgment. It exists to support it.

---

## How This Repository Uses ODD

This repository applies ODD in two layers:

- **Public-facing** ‚Äî this document and related writing explain the philosophy in human terms.
- **Canonical** ‚Äî internal reference documents capture constraints, decision rules, evidence standards, and failure modes.

The Canon is designed for orientation, not enforcement.

---

## On Variance and Learning

In AI-assisted development, outcomes are not deterministic. The same intent can produce different results depending on execution paths.

This site reflects that reality. Ideas are tested, observed, and sometimes retried before conclusions are drawn. What you see here is not a straight line‚Äîit's a record of learning under uncertainty.

---

## Where to Go Next

If you want to explore further:

- Read the **extended ODD Manifesto** in `/odd/manifesto.md`
- See how rigor scales in **Project Maturity & Progressive Governance**
- Browse the **Canon Index** to understand how decisions and verification are structured

Or skip the theory and look at projects as they are added over time.

---

> ODD is about preserving intent without freezing execution.
> The measure of success is not how elegant the artifact is, but whether the outcome holds up in the real world.


---

## Source: `odd/manifesto.md`

---
uri: klappy://odd/manifesto
title: "ODD Manifesto ‚Äî Extended"
audience: canon
exposure: nav
tier: 2
voice: neutral
stability: stable
tags: ["odd", "philosophy"]
---

# ODD Manifesto v1.1 (Extended)

> A governance framework for human-AI collaboration that optimizes learning, not execution.

## Description

Outcomes-Driven Development (ODD) operationalizes governance for human-AI collaboration. The core thesis: development is about defining outcomes, enforcing constraints, and verifying reality‚Äînot writing code. AI accelerates execution; governance preserves trust. The pillars include Prompt Over Code, KISS, DRY with Isolation, Consistency, Maintainability, Antifragile design, and Scalability. ODD treats restartability as a feature, applies progressive governance based on maturity (PoC ‚Üí Pilot ‚Üí Production), requires evidence over assertion, treats AI as accelerator not authority, demands falsifiable outcomes, prefers reversibility, and requires stop conditions. Memory is the bottleneck, not computation.

## Outline

- Purpose and Core Thesis
- Pillars (Operational Interpretation)
- Restartability Over Salvage
- Progressive Governance (Maturity-Aware)
- Evidence as the Gate
- Trust, Authority, and AI
- Outcomes Must Be Falsifiable
- Reversibility and Cost Awareness
- Stop Conditions
- Memory Is the Bottleneck
- Confidence, Risks, and Known Failure Modes

---

## Content

> ODD v1.1 ‚Äî Extended (Internal / Agent-Governance) ‚Üí for canon, MCP, agents (this file)

---

## üìå Purpose

This document operationalizes Outcomes-Driven Development as a governance framework for human‚ÄìAI collaboration.

It is designed to:
‚Ä¢ guide autonomous agents
‚Ä¢ enforce verification and evidence
‚Ä¢ scale judgment without repeating it
‚Ä¢ adapt rigor as projects mature

This version is not optimized for persuasion.
It is optimized for execution and enforcement.

---

## üéØ Core Thesis

The primary job of development is not writing code.
It is defining outcomes, enforcing constraints, and verifying reality.

AI accelerates execution.
Governance preserves trust.

---

## üìå Pillars (Operational Interpretation)

### Prompt Over Code
‚Ä¢ Intent is expressed declaratively.
‚Ä¢ Code is treated as ephemeral.
‚Ä¢ Regeneration is cheaper than preservation.

### KISS

‚Ä¢ Complexity is a liability.
‚Ä¢ Escalation requires evidence of failure.

### DRY (With Isolation)

‚Ä¢ Duplication is tolerated across bounded contexts.
‚Ä¢ Shared abstractions require proven reuse.

### Consistency

‚Ä¢ Behavioral predictability matters more than visual uniformity.
‚Ä¢ Consistency is scoped, not global.

### Maintainability

‚Ä¢ Systems must survive creator turnover.
‚Ä¢ Documentation and explicit tradeoffs are part of the product.

### Antifragile

‚Ä¢ Failure is assumed.
‚Ä¢ Recovery paths are preferred over prevention.
‚Ä¢ Learning velocity is a design constraint.

### Scalable

‚Ä¢ Growth must be bounded in:
‚Ä¢ cost
‚Ä¢ complexity
‚Ä¢ human attention
‚Ä¢ Scalability includes cognitive and operational load.

---

## üîÑ Restartability Over Salvage

ODD assumes that restarting from refined intent is often more effective than steering a system that has already drifted.

As systems grow, prompts accrete, assumptions harden, and local fixes compound. At a certain point, continued steering optimizes for preserving effort rather than improving outcomes.

Restarting is not failure.
Restarting is a recognition that:
‚Ä¢ intent has become clearer
‚Ä¢ constraints are better understood
‚Ä¢ evidence from prior attempts now exists

In an AI-accelerated environment, restarting is cheap.
Misalignment is expensive.

ODD therefore treats restartability as a design feature:
‚Ä¢ prompts are disposable
‚Ä¢ implementations are ephemeral
‚Ä¢ canon and intent persist

The goal is not to preserve artifacts, but to preserve learning.

A clean restart with better constraints is progress.

---

## üìä Progressive Governance (Maturity-Aware ODD)

ODD enforcement depends on project maturity.

Level 0 ‚Äî PoC / Exploration
‚Ä¢ Goal: learn quickly
‚Ä¢ Artifacts are non-authoritative
‚Ä¢ Verification demonstrates possibility
‚Ä¢ Over-governance is prohibited

Level 1 ‚Äî Pilot / Product
‚Ä¢ Goal: deliver value safely
‚Ä¢ Evidence and visual proof required
‚Ä¢ Tradeoffs must be explicit
‚Ä¢ Silent failure is unacceptable

Level 2 ‚Äî Production / Long-Term
‚Ä¢ Goal: sustain trust
‚Ä¢ Outcomes must be measurable
‚Ä¢ Observability, reversibility, and security are mandatory
‚Ä¢ Autonomous actions require stop conditions and human gates

Maturity must be stated explicitly.

---

## üìé Evidence as the Gate

Completion requires:
‚Ä¢ observed behavior
‚Ä¢ produced evidence
‚Ä¢ self-audit against constraints
‚Ä¢ explicit declaration of confidence and gaps

Assertions do not count as completion.

---

## ü§ñ Trust, Authority, and AI

AI is an accelerator, not an authority.
‚Ä¢ AI may propose and generate
‚Ä¢ AI may self-audit and verify
‚Ä¢ AI may not silently assume trust

Authority boundaries and escalation points must be explicit.

---

## üî¨ Outcomes Must Be Falsifiable

Outcomes are only valid if they can be:
‚Ä¢ observed
‚Ä¢ tested
‚Ä¢ disproven

Non-falsifiable outcomes are treated as goals, not success criteria.

---

## ‚ö†Ô∏è Reversibility and Cost Awareness

Prefer decisions that are:
‚Ä¢ cheap to undo
‚Ä¢ bounded in cost
‚Ä¢ limited in blast radius

Irreversible decisions require human approval.

---

## üõë Stop Conditions

Every autonomous loop must define:
‚Ä¢ success criteria
‚Ä¢ failure criteria
‚Ä¢ exit conditions

Endless optimization is a failure mode.

---

## üß† Memory Is the Bottleneck

AI didn't just make coding faster. It changed what's scarce.

In ODD, generated artifacts are abundant, but **durable intent** is not.
So the work shifts toward:

- preserving what was learned,
- verifying reality,
- discarding what cannot be trusted,
- and elevating only what repeatedly reduces future drag.

ODD stays legible by using **Progressive Elevation & Decay**:
most artifacts die at the Attempt/PRD layer; only proven patterns elevate into Contracts, Canon, and Decision Trace.

See:
- `/odd/appendices/progressive-elevation.md`
- `/docs/appendices/product-lanes.md`
- `/docs/appendices/epochs.md`

---

## üîó Relationship to Canon

‚Ä¢ ODD ‚Üí why
‚Ä¢ Constraints ‚Üí assumptions
‚Ä¢ Decision Rules ‚Üí how
‚Ä¢ Maturity Model ‚Üí when
‚Ä¢ Evidence Policies ‚Üí proof

Together, these form a complete governance layer.

---

## üí° Closing (Internal)

ODD is not a philosophy of optimism.

It is a discipline of restraint, verification, and curation‚Äî
designed for a world where generation is infinite, but trust is not.

---

## ‚úÖ Status

- ODD v1.1 finalized
- Public and internal views aligned
- Ready for MCP exposure and agent enforcement

---

## ‚ö†Ô∏è Confidence, Risks, and Known Failure Modes

(ODD v1.1 ‚Äî Internal Self-Assessment)

This section captures a snapshot assessment of how well Outcomes-Driven Development (ODD), as currently defined, aligns with its stated principles and where it is most vulnerable.

This is not a guarantee of correctness.
It is an explicit acknowledgment of uncertainty.

---

### Confidence Model

Confidence scores express current belief that ODD will behave as intended when applied thoughtfully.

Scale: 0.0‚Äì1.0
‚Ä¢ 0.9+ ‚Äî robust under most conditions
‚Ä¢ 0.7‚Äì0.85 ‚Äî strong, but watch for drift
‚Ä¢ 0.5‚Äì0.7 ‚Äî plausible, fragile under misuse
‚Ä¢ <0.5 ‚Äî likely misaligned without correction

Scores are expected to change as ODD is applied in practice.

---

### Principle-Level Confidence Snapshot

**Prompt Over Code / Convention Over Configuration**
Confidence: 0.80

Why this is strong
‚Ä¢ ODD treats intent, constraints, and outcomes as first-class artifacts.
‚Ä¢ Canonical resources replace brittle, repeated prompts with stable conventions.

Primary risks
‚Ä¢ Conventions silently becoming configuration sprawl.
‚Ä¢ Clients inventing ad hoc mappings instead of using shared conventions.

Failure mode
‚Ä¢ "Prompt over code" degenerates into "prompt + hidden config everywhere."

---

**KISS (Keep It Simple, Stupid)**
Confidence: 0.75

Why this is strong
‚Ä¢ ODD avoids embedding workflows or agent loops.
‚Ä¢ Complexity is deferred intentionally.

Primary risks
‚Ä¢ Meta-layers (manifests, indices, maturity flags) accumulating unchecked.
‚Ä¢ Over-abstracting governance before it proves necessary.

Failure mode
‚Ä¢ Governance becomes heavier than the systems it governs.

---

**DRY (With Isolation)**
Confidence: 0.70

Why this is strong
‚Ä¢ Canon centralizes worldview and defaults.
‚Ä¢ Single-inventory patterns reduce duplication.

Primary risks
‚Ä¢ Multiple parallel indices drifting out of sync.
‚Ä¢ Reuse pressure creating brittle shared abstractions too early.

Failure mode
‚Ä¢ "One source of truth" becomes "many partial truths."

---

**Consistency**
Confidence: 0.65

Why this is weaker
‚Ä¢ Consistency depends on discipline, not tooling.
‚Ä¢ Naming, casing, and URI patterns are easy to drift over time.

Primary risks
‚Ä¢ Small inconsistencies compounding across resources and clients.
‚Ä¢ Human tolerance masking slow degradation.

Failure mode
‚Ä¢ The system remains logically sound but ergonomically frustrating.

---

**Maintainability**
Confidence: 0.70

Why this is strong
‚Ä¢ Separation of stable principles from evolving operations.
‚Ä¢ Explicit maturity model prevents premature hardening.

Primary risks
‚Ä¢ Manual maintenance of inventories becoming burdensome.
‚Ä¢ Version semantics implied but not enforced.

Failure mode
‚Ä¢ Canon becomes respected but stale.

---

**Antifragile**
Confidence: 0.60

Why this is intentionally cautious
‚Ä¢ Antifragility depends on real-world stress, not theory.
‚Ä¢ Recovery paths are assumed, not yet proven.

Primary risks
‚Ä¢ MCP or tooling layers becoming hidden single points of failure.
‚Ä¢ Ephemerality mistaken for disposability of meaning.

Failure mode
‚Ä¢ System recovers technically but loses trust socially.

---

**Scalable**
Confidence: 0.70

Why this is strong
‚Ä¢ ODD scales conceptually: more resources do not require new rules.
‚Ä¢ Governance grows linearly, not exponentially.

Primary risks
‚Ä¢ Human cognitive load becoming the true bottleneck.
‚Ä¢ Discovery/search degrading without deliberate tooling later.

Failure mode
‚Ä¢ System scales in size but not in usability.

---

### Cross-Cutting Risks

**Premature Formalization**

ODD is vulnerable to being "locked in" too early, reducing exploration.

**False Authority**

Well-written governance can be mistaken for correctness without evidence.

**Silent Drift**

Small deviations, left unnamed, can erode trust over time.

---

### Intended Use of This Section

This section exists to:
‚Ä¢ prevent ideological hardening
‚Ä¢ make risks discussable
‚Ä¢ encourage re-evaluation
‚Ä¢ model intellectual humility

It is expected to change.

---

### Re-evaluation Philosophy

ODD should be reassessed when:
‚Ä¢ it is applied to real production systems
‚Ä¢ autonomous agents operate for extended periods
‚Ä¢ failure modes surface that are not addressed here

Confidence should be updated based on evidence, not optimism.

---

Closing (Internal)

ODD is not complete.

It is a living attempt to govern creativity, autonomy, and trust in a world where generation is cheap and certainty is not.

Its strength is not that it claims to be right‚Äî
but that it makes being wrong visible early.

For common failure modes and practical misapplications of ODD, see _Misuse Patterns_ and _Prompt Architecture_ in the ODD appendices.

---

Status
‚Ä¢ ODD v1.1 Extended updated
‚Ä¢ Confidence scoring and failure modes explicitly documented
‚Ä¢ Fully aligned with Canon Index confidence model

---


---

## Source: `odd/cognitive-partitioning.md`

---
uri: klappy://odd/cognitive-partitioning
title: "Cognitive Partitioning"
audience: docs
exposure: nav
tier: 1
voice: neutral
stability: evolving
tags: ["odd", "cognition", "scaling", "decision-load"]
---

# Cognitive Partitioning

> Why reasoning systems must divide under pressure, just like execution systems do.

## Description

As systems accumulate tools, context, and responsibilities, the decision surface of a
single reasoning entity expands faster than its reliability.

Cognitive Partitioning names this constraint and explains why isolating reasoning
responsibilities becomes necessary as systems scale. The concept is universal and
does not prescribe any specific implementation.

## Outline

- The failure mode
- The underlying constraint
- Analogy: hiring too early
- Relationship to other ODD concepts
- Non-goals

---

## The Failure Mode

When a reasoning system has access to too many valid actions, it begins to fail
not from lack of capability, but from excess choice.

Symptoms include hesitation, inconsistent behavior, over-exploration, and repeated
clarification loops ‚Äî even when the tools themselves are "correct."

---

## The Constraint

Decision complexity grows faster than execution capability.

Past a threshold, adding more tools can degrade reliability unless reasoning scope
is reduced.

---

## Analogy: Hiring Too Early

The same failure mode appears in early-stage teams.

Effective startups rarely hire a large staff upfront and then decide what each
person should do. Instead, they operate with a small, generalist core until
specific pain becomes visible ‚Äî missed deadlines, overloaded founders, or repeated
failures in a narrow area.

Hiring occurs in response to pressure, not anticipation.

Teams that hire ahead of demonstrated need experience the same symptoms as
overloaded reasoning systems:
- unclear ownership
- duplicated or conflicting work
- excessive coordination
- founders managing people instead of outcomes

Cognitive Partitioning follows the same rule. Reasoning capacity is expanded only
when existing structures can no longer reliably absorb the load.

---

## Relationship to Other ODD Concepts

Product Lanes partition execution to preserve evidence integrity.
Cognitive Partitioning applies the same pressure logic to reasoning itself.

---

## Non-goals

This document does not define:
- specific agents
- specific tools or MCP servers
- orchestration frameworks
- required workflows

It explains why systems evolve toward isolation as complexity grows.

---

## See Also

- [Product Lanes](/docs/appendices/product-lanes.md) ‚Äî Execution partitioning under pressure
- [ODD Misuse Patterns](/odd/misuse-patterns.md) ‚Äî Common failure modes (diagnostic)


---

## Source: `odd/appendices/README.md`

---
uri: klappy://odd/appendices
title: "ODD Appendices (Portable)"
audience: canon
exposure: nav
tier: 2
voice: neutral
stability: evolving
tags: ["odd", "appendices", "index", "portable"]
---

# ODD Appendices (Portable)

Extended concepts that deepen understanding without introducing enforcement. These are diagnostic and orientation documents, not requirements.

> **Note:** Implementation-specific appendices have been moved to `/docs/appendices/`. This folder contains only portable methodology that can apply to any ODD-following repository.

---

## Contents

| File | Title | Summary |
|------|-------|---------|
| `alignment-reviews.md` | Alignment Reviews | Periodic evaluation of the ODD system itself to detect drift between stated intent, implemented process, and observed outcomes. |
| `evolution-not-automation.md` | Evolution, Not Automation | This system optimizes learning, not execution. Humans stay in the loop. |
| `failure-driven-modularity.md` | Failure-Driven Modularity | Modular boundaries are introduced only after repeated failure to regenerate from spec. Modularity is an outcome of failure, not a prerequisite. |
| `media-as-learning-layer.md` | Media as a Learning Layer | Media reduces cognitive load over stable written content. Canonical truth lives in text. |
| `progressive-elevation.md` | Progressive Elevation & Decay | The five-layer portability model: how artifacts move from ephemeral attempts to durable canon through strict elevation criteria. Most should decay; few should elevate. |
| `quantum-development.md` | Quantum Development | Why multiple attempts against the same PRD are sometimes necessary before changing the PRD itself. |
| `visual-evolution.md` | Visual Evolution | Visual systems evolve independently from products through versioned visual interfaces. |

---

## Implementation-Specific Appendices

The following have been moved to `/docs/appendices/` as they contain klappy.dev-specific implementation details:

- `attempt-lifecycle.md` ‚Äî Attempt folder structure, CLI commands, META.json schema
- `compilation.md`, `compiled-memory.md`, `compilation-targets.md` ‚Äî Compilation paths and tooling
- `epochs.md` ‚Äî E0003 evidence requirements with Cloudflare specifics
- `evidence.md`, `deploy-evidence.md`, `online-evidence.md` ‚Äî Evidence path structure
- `lane-build-layout.md`, `lane-implementation-surfaces.md` ‚Äî Lane-specific paths
- `product-lanes.md` ‚Äî Specific lane names (website, ai-navigation, agent-skill)
- `repo-topology.md`, `repo-truth.md`, `repo-truth-audit.md` ‚Äî Specific folder structures
- `canonical-compression.md`, `memory-architecture.proposed.md` ‚Äî Compilation and memory paths

---

## When to Read What

**Understanding ODD methodology?** Start with these portable appendices.

**Implementing ODD in your own repo?** Use these as the conceptual foundation.

**Understanding klappy.dev specifics?** Read `/docs/appendices/` instead.

---

## Relationship to Canon

These appendices extend the core canon documents:

- `constraints.md` ‚Üí appendices explain edge cases
- `definition-of-done.md` ‚Üí evidence philosophy here, evidence procedures in docs
- `odd/manifesto.md` ‚Üí appendices operationalize philosophy


---

## Source: `odd/decisions/README.md`

---
uri: klappy://odd/decisions
title: "ODD Conceptual Decisions"
audience: canon
exposure: nav
tier: 1
voice: neutral
stability: stable
tags: ["odd", "decisions", "conceptual", "philosophy"]
---

# ODD Conceptual Decisions

> Decisions about ODD's mental model and conceptual architecture.

This folder contains decisions about ODD itself ‚Äî the philosophy, not any specific implementation.

---

## Conceptual Decisions (This Folder)

| ID | Decision | Summary |
|----|----------|---------|
| [D0001](./D0001-three-tier-conceptual-hierarchy.md) | Three-Tier Conceptual Hierarchy | ODD separates universal principles ‚Üí program constraints ‚Üí implementation details |

---

## Two Types of Decisions

| Location | Contains | Example |
|----------|----------|---------|
| `/odd/decisions/` | Decisions about ODD's conceptual architecture | "ODD is a three-tier hierarchy" |
| `/docs/decisions/` | Decisions about this implementation | "prod branch is production" |

---

## The Principle

> **Conceptual architecture lives in canon. Implementation decisions live in docs.**

The three-tier model (ODD ‚Üí Canon ‚Üí Docs) is itself captured in D0001.

---

## See Also

- [D0001: Three-Tier Conceptual Hierarchy](./D0001-three-tier-conceptual-hierarchy.md)
- `/docs/decisions/README.md` ‚Äî Implementation decision index
- `/odd/contract.md` ‚Äî ODD System Contract


---

## Source: `canon/odd/appendices/tool-specialization.md`

---
uri: klappy://canon/odd/tool-specialization
title: "Tool Specialization"
audience: canon
exposure: nav
tier: 2
voice: neutral
stability: evolving
tags: ["odd", "pattern", "tools", "decision-complexity"]
---

# Tool Specialization

> A general pattern for preserving reliability as tool availability increases.

## Description

When systems accumulate many tools or actions, generalist reasoning becomes
unreliable. The Tool Specialization pattern isolates tool usage behind narrowly
scoped reasoning units, reducing decision complexity while preserving capability.

This pattern captures invariants and tradeoffs without prescribing a specific
implementation.

## Outline

- Context
- The pattern
- Invariants
- Tradeoffs
- Non-goals

---

## Context

This pattern emerges when adding tools increases confusion, misfires, or decision
paralysis instead of effectiveness.

Typical triggers:
- a growing tool menu that the "main" agent uses inconsistently
- repeated tool misuse despite prompt reminders
- correct tools, wrong selection timing
- tool choice dominates reasoning time

---

## The Pattern

Assign responsibility for a narrow set of tools or actions to a dedicated reasoning
unit with constrained scope and explicit outputs.

The goal is not "smarter" reasoning.
The goal is making tool-use boring and reliable.

---

## Invariants

- Capability grows faster than reliability without isolation
- Isolation precedes orchestration
- Specialization reduces decision load, not intelligence
- Outputs must be explicit and promotable

---

## Tradeoffs

- Increased coordination overhead
- Additional interfaces to maintain
- Risk of premature specialization if created before pressure is visible

---

## Non-goals

This pattern does not define:
- an agent framework
- a required topology
- how sub-agents are implemented in any specific repo

---

## See Also

- [Cognitive Partitioning](/odd/cognitive-partitioning.md) ‚Äî Universal concept
- [Canonical Compression](/docs/appendices/canonical-compression.md) ‚Äî Reduce reasoning surface area (context limits)


---

## Source: `canon/constraints.md`

---
uri: klappy://canon/constraints
title: "Constraints"
audience: canon
exposure: nav
tier: 1
voice: first_person
stability: stable
tags: ["constraints", "assumptions"]
---

# Constraints

> Design defaults and assumptions that shape how systems are built.

## Description

Constraints define the baseline assumptions and design defaults applied to most work. They cover offline-first design, long-term maintainability, interoperability, stateless architectures, AI as accelerator (not authority), evidence over assertion, contextual UX, ephemeral artifacts, and explicit tradeoffs. Each constraint includes what is assumed, why it matters, what it forces, and when it does not apply. These are not universal best practices but reflect specific environments and problems.

## Outline

- Offline-First (Default)
- Long Timelines & Changing Ownership
- Maintainability Over Cleverness
- Interoperability Over Feature Completeness
- Stateless or Low-State by Default
- AI as Accelerator, Not Authority
- Evidence Over Assertion
- UX Is Contextual, Not Universal
- Ephemeral Artifacts Are Acceptable
- Explicit Tradeoffs

---

## Content

**Canon v0.1**

> This is written in first person, website-ready, and structured so agents can reliably translate it into neutral/system constraints at runtime.

Each constraint includes:
- what I assume
- why it matters
- what it forces
- when it doesn't apply

That last part is critical to avoid dogma.

This page documents the defaults and constraints I design under most often.
They are not universal best practices. They reflect the environments and problems I regularly work in.

Unless explicitly stated otherwise, these constraints should be assumed to apply.

---

## 1. Offline-First (Default)

I design as if network connectivity is unreliable, intermittent, or unavailable.

**Why this matters**

Many of the contexts I work in:
‚Ä¢ have poor or inconsistent internet access
‚Ä¢ require field use
‚Ä¢ cannot assume cloud availability

Designs that fail offline tend to fail catastrophically.

**What this forces**
‚Ä¢ Core functionality must work without a network
‚Ä¢ Data is stored locally first
‚Ä¢ Synchronization is opportunistic, not assumed
‚Ä¢ Conflicts are expected and must be resolvable

**When this does not apply**
‚Ä¢ Short-lived internal tools
‚Ä¢ One-off demos where offline support would distort the experiment
‚Ä¢ Explicitly cloud-only systems (must be stated)

---

## 2. Long Timelines & Changing Ownership

I assume systems will live longer than their original creators and will change hands.

**Why this matters**

Many projects:
‚Ä¢ span years, not months
‚Ä¢ outlast funding cycles
‚Ä¢ rotate maintainers or organizations

Systems that assume stable ownership tend to rot.

**What this forces**
‚Ä¢ Clear separation of concerns
‚Ä¢ Minimal hidden state
‚Ä¢ Explicit documentation as part of the product
‚Ä¢ Avoidance of "tribal knowledge" dependencies

**When this does not apply**
‚Ä¢ Throwaway prototypes
‚Ä¢ Time-boxed experiments with a defined end date

---

## 3. Maintainability Over Cleverness

I default to solutions that are easy to understand, modify, and repair.

**Why this matters**

Maintenance cost usually exceeds build cost, especially over long timelines.

**What this forces**
‚Ä¢ Preference for simple, boring solutions
‚Ä¢ Avoidance of unnecessary abstractions
‚Ä¢ Clear tradeoffs documented when complexity is introduced

**When this does not apply**
‚Ä¢ Exploratory research prototypes
‚Ä¢ Performance-critical paths where simplicity is insufficient

---

## 4. Interoperability Over Feature Completeness

I prioritize systems that can work with others over systems that try to do everything.

**Why this matters**

Closed or tightly coupled systems:
‚Ä¢ limit collaboration
‚Ä¢ increase lock-in
‚Ä¢ age poorly

Interoperable systems survive organizational change.

**What this forces**
‚Ä¢ Preference for open formats and protocols
‚Ä¢ Loose coupling between components
‚Ä¢ Clear interfaces instead of shared internals

**When this does not apply**
‚Ä¢ Highly specialized tools with no external integration needs
‚Ä¢ Temporary scaffolding code

---

## 5. Stateless or Low-State by Default

I default to stateless or low-state architectures where possible.

**Why this matters**

State increases:
‚Ä¢ complexity
‚Ä¢ failure modes
‚Ä¢ recovery cost

Stateless systems are easier to reason about and recover.

**What this forces**
‚Ä¢ Explicit state boundaries
‚Ä¢ Externalized persistence where necessary
‚Ä¢ Clear lifecycle management

**When this does not apply**
‚Ä¢ Systems whose core value is stateful (e.g., editors, long-running workflows)
‚Ä¢ Performance-critical stateful processes (must be justified)

---

## 6. AI as Accelerator, Not Authority

I treat AI as a tool to accelerate thinking and execution, not as a source of truth.

**Why this matters**

AI systems:
‚Ä¢ hallucinate
‚Ä¢ optimize for plausibility, not correctness
‚Ä¢ require human judgment

Unverified AI output is a liability.

**What this forces**
‚Ä¢ Human-defined outcomes
‚Ä¢ Verification and evidence requirements
‚Ä¢ Explicit refusal when confidence is not warranted

**When this does not apply**
‚Ä¢ None. This constraint is always in effect.

---

## 7. Evidence Over Assertion

I do not consider work complete unless it is verified with evidence.

**Why this matters**

Assertions like "it works" are unreliable without proof.

**What this forces**
‚Ä¢ Running the system
‚Ä¢ Observing behavior
‚Ä¢ Producing visual or test evidence

**When this does not apply**
‚Ä¢ Purely conceptual or theoretical work (must be labeled as such)

---

## 8. UX Is Contextual, Not Universal

I do not assume a single UX pattern works everywhere.

**Why this matters**

Users vary by:
‚Ä¢ language
‚Ä¢ culture
‚Ä¢ technical experience
‚Ä¢ environment

Universal UX claims often hide bias.

**What this forces**
‚Ä¢ Context-specific design decisions
‚Ä¢ Willingness to diverge from mainstream patterns
‚Ä¢ Clear explanation of UX tradeoffs

**When this does not apply**
‚Ä¢ Internal tools for a well-defined, homogeneous user group

---

## 9. Ephemeral Artifacts Are Acceptable

I assume many outputs (code, UI, builds) are temporary.

**Why this matters**

AI makes regeneration cheap. Maintaining everything forever is unnecessary.

**What this forces**
‚Ä¢ Focus on outcomes over artifacts
‚Ä¢ Willingness to discard and regenerate
‚Ä¢ Durable principles instead of durable repos

**When this does not apply**
‚Ä¢ Canonical data
‚Ä¢ Long-term user content
‚Ä¢ Legal or compliance artifacts

---

## 10. Explicit Tradeoffs

I expect tradeoffs to be named, not hidden.

**Why this matters**

Every decision excludes alternatives. Unspoken tradeoffs cause confusion later.

**What this forces**
‚Ä¢ Short explanations of why choices were made
‚Ä¢ Acknowledgment of downsides
‚Ä¢ Easier future revision

**When this does not apply**
‚Ä¢ Truly trivial decisions

---

## üí° Closing Note

These constraints define how I default, not how everyone should build.

Agents and collaborators should:
‚Ä¢ assume these constraints apply
‚Ä¢ translate them into neutral/system requirements
‚Ä¢ explicitly note when a constraint is overridden or does not apply

---

## ‚úÖ Status

- Canon v0.1 ‚Äî Constraints complete
- Ready to proceed to Canon v0.1 ‚Äî Decision Rules


---

## Source: `canon/decision-rules.md`

---
uri: klappy://canon/decision-rules
title: "Decision Rules"
audience: canon
exposure: nav
tier: 2
voice: first_person
stability: stable
tags: ["decision-rules", "heuristics"]
---

# Decision Rules

> Heuristics for choosing between valid options when designing systems.

## Description

Decision rules describe how decisions are made when multiple valid options exist. They complement Constraints by answering "how I choose." Rules include: outcomes before implementation, borrow-bend-break-build progression, KISS, DRY with isolation, explicit state, recoverability over perfection, visible tradeoffs, optimizing for the next maintainer, UI carrying explanation, verification before completion, escalation only when defaults fail, admitting uncertainty early, preferring one-shot builds over steering misses, and hard-coding protocols (not domain tables).

## Outline

- Outcomes Before Implementation
- Borrow, Bend, Break, Build
- Simplicity Wins (KISS)
- DRY, But Not at the Cost of Isolation
- Prefer Explicit State
- Favor Recoverability Over Perfection
- Make Tradeoffs Visible Early
- Optimize for the Next Maintainer
- UI Should Carry the Explanation
- If It Cannot Be Verified, It Is Not Done
- Escalate Only When Defaults Fail
- Say "I Don't Know" Early
- Prefer One-Shot Builds
- Hard-Code Protocols, Not Domain Tables

---

## Content

**Canon v0.1**

> This complements the Constraints by answering "how I choose" when multiple valid options exist.

These rules describe how I tend to make decisions when designing systems.
They are not absolute laws. They are defaults I apply unless there is a clear reason not to.

If a rule is overridden, I expect the reason to be stated explicitly.

---

## 1. Outcomes Before Implementation

I define the outcome I care about before choosing tools, architectures, or code.

**How I apply this**
‚Ä¢ I ask what problem is actually being solved
‚Ä¢ I avoid committing to implementation details too early
‚Ä¢ I prefer deleting work that doesn't move the outcome forward

**Signals this rule was violated**
‚Ä¢ The solution is impressive but unclear in purpose
‚Ä¢ Success criteria are vague or missing
‚Ä¢ The system "works" but doesn't help anyone

---

## 2. Borrow ‚Üí Bend ‚Üí Break ‚Üí Build

I follow a progression when deciding how much to create from scratch.

**The order:**

1. **Borrow** ‚Äî Use an existing tool as-is
2. **Bend** ‚Äî Extend or configure an existing tool
3. **Break** ‚Äî Fork or partially replace an existing tool
4. **Build** ‚Äî Create something new from components

**How I apply this**
‚Ä¢ I start at Borrow and justify moving down the list
‚Ä¢ Building from scratch requires explicit justification

**Signals this rule was violated**
‚Ä¢ Reinventing something stable and well-understood
‚Ä¢ Forking without a clear maintenance plan

---

## 3. Simplicity Wins by Default (KISS)

I choose the simplest solution that plausibly works.

**How I apply this**
‚Ä¢ I reject unnecessary abstraction
‚Ä¢ I prefer readable code over clever code
‚Ä¢ I add complexity only when simplicity demonstrably fails

**Signals this rule was violated**
‚Ä¢ Explanations are longer than the code
‚Ä¢ Only the original author understands the system

---

## 4. DRY, But Not at the Cost of Isolation

I avoid duplication, but not if it creates brittle coupling.

**How I apply this**
‚Ä¢ I allow duplication across bounded contexts
‚Ä¢ I extract shared logic only when reuse is proven
‚Ä¢ I avoid "god modules" shared by everything

**Signals this rule was violated**
‚Ä¢ Small changes cause widespread breakage
‚Ä¢ Teams are blocked waiting on shared components

---

## 5. Prefer Explicit State Over Implicit State

I choose designs where state is visible, named, and bounded.

**How I apply this**
‚Ä¢ I avoid hidden global state
‚Ä¢ I make lifecycle and ownership explicit
‚Ä¢ I prefer passing state over reaching for it

**Signals this rule was violated**
‚Ä¢ Bugs depend on execution order
‚Ä¢ Restarting the system produces surprising behavior

---

## 6. Favor Recoverability Over Perfection

I prefer systems that fail safely and recover easily over systems that try to prevent all failure.

**How I apply this**
‚Ä¢ I design for partial failure
‚Ä¢ I assume components will break
‚Ä¢ I prefer restartable, replayable processes

**Signals this rule was violated**
‚Ä¢ A single failure takes everything down
‚Ä¢ Recovery requires deep expertise or manual intervention

---

## 7. Make Tradeoffs Visible Early

I name tradeoffs as part of the design, not as a postmortem.

**How I apply this**
‚Ä¢ I document why a choice was made
‚Ä¢ I acknowledge what the choice sacrifices
‚Ä¢ I leave breadcrumbs for future maintainers

**Signals this rule was violated**
‚Ä¢ Future changes require archaeology
‚Ä¢ Decisions feel arbitrary in hindsight

---

## 8. Optimize for the Next Maintainer

I assume the next person to touch the system is not me.

**How I apply this**
‚Ä¢ I favor clarity over personal preference
‚Ä¢ I document non-obvious decisions
‚Ä¢ I avoid designs that require constant explanation

**Signals this rule was violated**
‚Ä¢ The system works but no one wants to touch it
‚Ä¢ Knowledge exists only in conversations or chat logs

---

## 9. UI Should Carry the Explanation

I prefer showing over telling, especially in user-facing systems.

**How I apply this**
‚Ä¢ I let interfaces demonstrate behavior
‚Ä¢ I keep explanatory text short
‚Ä¢ I ask permission before going deep

**Signals this rule was violated**
‚Ä¢ Long explanations compensate for confusing UX
‚Ä¢ Users need documentation to complete basic tasks

---

## 10. If It Can't Be Verified, It Isn't Done

I do not consider work complete unless it is verified.

**How I apply this**
‚Ä¢ I run the system
‚Ä¢ I observe behavior directly
‚Ä¢ I require visual or test evidence

**Signals this rule was violated**
‚Ä¢ Confidence is based on reasoning alone
‚Ä¢ Bugs are discovered by users instead of builders

---

## 11. Escalate Only When Defaults Fail

I start with defaults and escalate only when necessary.

**How I apply this**
‚Ä¢ I try the obvious solution first
‚Ä¢ I gather evidence before increasing complexity
‚Ä¢ I treat escalation as a signal, not a failure

**Signals this rule was violated**
‚Ä¢ Overengineering early
‚Ä¢ Complex solutions to simple problems

---

## 12. Say "I Don't Know" Early

I prefer admitting uncertainty to pretending confidence.

**How I apply this**
‚Ä¢ I name unknowns explicitly
‚Ä¢ I design experiments to reduce uncertainty
‚Ä¢ I avoid locking in assumptions prematurely

**Signals this rule was violated**
‚Ä¢ Decisions are justified with vague confidence
‚Ä¢ Surprises appear late and expensively

---

## 13. Prefer One-Shot Builds; Don't Steer a Miss

I prefer fixing the asset (PRD, constraints, inputs) and re-running clean over steering a multi-turn miss.

**How I apply this**
‚Ä¢ I treat a failed execution path as signal, not a trajectory to nurse back to health
‚Ä¢ If context decays, I restart with corrected inputs rather than accumulating patches
‚Ä¢ I preserve the attempt as evidence, then begin a new attempt independently

**Signals this rule was violated**
‚Ä¢ "Just one more tweak" turns into extended steering
‚Ä¢ The system only works if the same person keeps nudging it
‚Ä¢ The final outcome cannot be reproduced from a clean start

---

## 14. Don't Hard-Code Domain Tables; Hard-Code Protocol Contracts

I avoid hard-coding domain lookups that can be derived, fetched, or updated without code changes.

I do hard-code protocol contracts that define interoperability:
- types
- schemas
- action primitives
- allowed states and transitions

**How I apply this**
‚Ä¢ If it's "data," I prefer it to live in content, configuration, or a source of truth
‚Ä¢ If it's "interface," I prefer it to be explicit and enforced in code

**Signals this rule was violated**
‚Ä¢ Large in-code tables that drift from reality (e.g., enumerations maintained by hand)
‚Ä¢ Domain updates require redeploys without justification
‚Ä¢ Integrations fail because the "contract" was implicit or inconsistent

---

## üí° Closing Note

These rules describe how I tend to decide, not how decisions must always be made.

Agents and collaborators should:
‚Ä¢ apply these rules by default
‚Ä¢ translate them into neutral/system logic
‚Ä¢ state clearly when a rule is overridden and why

---

## ‚úÖ Status

- Canon v0.1 ‚Äî Decision Rules complete
- Ready to proceed to Canon v0.1 ‚Äî Definition of Done & Evidence Policy


---

## Source: `canon/definition-of-done.md`

---
uri: klappy://canon/definition-of-done
title: "Definition of Done & Evidence Policy"
audience: canon
exposure: nav
tier: 1
voice: first_person
stability: semi_stable
tags: ["definition-of-done", "evidence"]
---

# Definition of Done & Evidence Policy

> The enforcement backbone that defines what "complete" means.

## Description

This policy defines completion requirements for all work: code, UI, architecture, automation, and AI-assisted outputs. Work is only done when it includes a change description, verification performed, observed behavior, evidence produced, and self-audit completed. Evidence must demonstrate actual behavior (screenshots, recordings, rendered output, test logs) and be produced after the change. Visual verification is required for UI work. The policy covers partial completion handling, explicit exceptions, and agent responsibilities.

## Outline

- Core Principle: Verified with evidence
- Definition of Done (5 requirements)
- Evidence Requirements
- Visual Verification (Preferred)
- Verification Must Be Performed
- Self-Audit Requirement
- What Does Not Count as Done
- Partial Completion
- Explicit Exceptions
- Agent Responsibility

---

## Content

**Canon v0.1**

> This is the enforcement backbone of the canon. It replaces repeated QA reminders with a clear, reusable contract.

This page defines what I mean when I say work is "done."
If these conditions are not met, the work is not complete, regardless of confidence or explanation.

This policy applies to:
‚Ä¢ code
‚Ä¢ UI
‚Ä¢ architecture
‚Ä¢ automation
‚Ä¢ AI-assisted outputs

---

## üìå Core Principle

I do not consider work complete unless it is verified with evidence.

Reasoning alone is insufficient.
Assertions like "this should work" or "this is correct" do not count as completion.

---

## üìã Definition of Done (DoD)

A task is only considered done when all of the following are present:

1. **Change Description** ‚Äî What changed, where, and why.
2. **Verification Performed** ‚Äî What was run or checked to verify the change.
3. **Observed Behavior** ‚Äî What actually happened when the system was run.
4. **Evidence Produced** ‚Äî Proof that the behavior matches the intended outcome.
5. **Self-Audit Completed** ‚Äî A brief audit against constraints and decision rules.

If any of these is missing, the task is incomplete.

---

## üìé Evidence Requirements

Evidence must demonstrate actual behavior, not expected behavior.

Acceptable evidence includes one or more of the following:
‚Ä¢ screenshots
‚Ä¢ short screen recordings (10‚Äì30 seconds)
‚Ä¢ rendered output files
‚Ä¢ test output logs
‚Ä¢ DOM snapshots or structured UI state captures

Evidence must be:
‚Ä¢ produced after the change
‚Ä¢ specific to the task
‚Ä¢ clearly labeled

---

## üëÅÔ∏è Visual Verification (Preferred)

If the work affects:
‚Ä¢ UI
‚Ä¢ interaction
‚Ä¢ layout
‚Ä¢ user flow
‚Ä¢ visible state

Then visual proof is required.

**What counts as visual proof**
‚Ä¢ screenshot showing the correct state
‚Ä¢ short recording demonstrating the interaction
‚Ä¢ before/after comparison when relevant

If visual proof cannot be produced, the reason must be stated explicitly.

---

## üî¨ Verification Must Be Performed

I expect the system to be run or exercised, not just reasoned about.

Verification may include:
‚Ä¢ running a dev server
‚Ä¢ executing tests
‚Ä¢ loading a page
‚Ä¢ triggering a workflow
‚Ä¢ simulating offline/online transitions

If verification cannot be performed (missing environment, credentials, etc.), this must be stated clearly, along with a proposed alternative.

---

## üîç Self-Audit Requirement

Each completed task must include a short self-audit covering:
‚Ä¢ intended outcome
‚Ä¢ relevant constraints applied
‚Ä¢ relevant decision rules followed
‚Ä¢ known tradeoffs
‚Ä¢ remaining risks or unknowns

The purpose is reflection and traceability, not bureaucracy.

---

## ‚ö†Ô∏è What Does Not Count as Done

The following do not qualify as completion:
‚Ä¢ "It compiles"
‚Ä¢ "The logic is sound"
‚Ä¢ "I reviewed the code"
‚Ä¢ "This should work"
‚Ä¢ "I didn't have time to test"

These may be intermediate states, but they are not "done."

---

## ‚è≥ Partial Completion

If work is partially complete, it must be labeled as such.

A valid partial completion includes:
‚Ä¢ what was attempted
‚Ä¢ what was verified
‚Ä¢ what could not be verified
‚Ä¢ what remains

Ambiguity is worse than incompleteness.

---

## üö´ Explicit Exceptions

This policy may be relaxed only when explicitly stated, such as for:
‚Ä¢ conceptual design discussions
‚Ä¢ theoretical analysis
‚Ä¢ early ideation

In those cases, the output must be clearly labeled "unverified".

---

## ü§ñ Agent Responsibility

Agents and collaborators are expected to:
‚Ä¢ retrieve this policy before claiming completion
‚Ä¢ translate it into neutral/system requirements
‚Ä¢ enforce it against their own output
‚Ä¢ refuse to claim "done" without evidence

If evidence cannot be produced, the correct response is:

"This is not complete yet."

---

## üí° Closing Note

This policy exists to:
‚Ä¢ prevent false confidence
‚Ä¢ reduce rework
‚Ä¢ replace repeated QA reminders
‚Ä¢ make outcomes trustworthy

It is not meant to slow work down.
It is meant to stop work from being incorrectly declared finished.

---

## ‚úÖ Status

- Canon v0.1 ‚Äî Definition of Done & Evidence Policy complete
- Ready to proceed to Canon v0.1 ‚Äî Self-Audit Checklist


---

## Source: `canon/self-audit.md`

---
uri: klappy://canon/self-audit
title: "Self-Audit Checklist"
audience: canon
exposure: nav
tier: 2
voice: first_person
stability: evolving
tags: ["self-audit", "verification"]
---

# Self-Audit Checklist

> A reflection layer that makes the Definition of Done actionable.

## Description

The self-audit checklist defines how work should be self-reviewed before claiming completion. It covers nine areas: intended outcome, constraints applied, decision rules followed, verification performed, evidence produced, UX and behavior check, tradeoffs and risks, maintainability check, and confidence level. Minimum acceptable completion requires a stated outcome, at least one verification step, at least one piece of evidence, and acknowledgment of tradeoffs or unknowns. This replaces repeated back-and-forth questions about whether work was actually run and verified.

## Outline

- Intended Outcome
- Constraints Applied
- Decision Rules Followed
- Verification Performed
- Evidence Produced
- UX & Behavior Check
- Tradeoffs & Risks
- Maintainability Check
- Confidence Level
- Minimum Acceptable Completion
- Agent Expectations

---

## Content

**Canon v0.1**

> This is the reflection and enforcement layer that makes the Definition of Done actionable without turning you into a QA manager.

This checklist defines how I expect work to be self-reviewed before it is considered complete.

The purpose is not bureaucracy.
The purpose is to catch obvious failures before someone else does.

Every completed task must include a filled version of this checklist.

---

## üìå Core Principle

I expect builders‚Äîhuman or AI‚Äîto audit their own work against stated outcomes, constraints, and evidence.

If an item cannot be answered, that is a signal‚Äînot a failure.

---

## üìã Self-Audit Checklist

### 1. Intended Outcome

   ‚Ä¢ What outcome was this work intended to achieve?
   ‚Ä¢ How will someone know if that outcome was achieved?

---

### 2. Constraints Applied

- Which constraints were relevant to this task?
- (e.g., offline-first, maintainability, interoperability)
- Were any default constraints intentionally overridden?
- If yes, why?

---

### 3. Decision Rules Followed

- Which decision rules guided the approach?
- (e.g., Borrow‚ÜíBend‚ÜíBreak‚ÜíBuild, KISS, explicit tradeoffs)
- Were there moments where a different rule could have been applied?
- Why was it not?

---

### 4. Verification Performed

- What was run or exercised to verify the work?
- What behavior was directly observed?

---

### 5. Evidence Produced

- What evidence proves the behavior occurred?
  - screenshots
  - recordings
  - logs
  - rendered output
- Where can this evidence be found?

---

### 6. UX & Behavior Check (If Applicable)

- Does the UI or interaction behave as expected?
- Is the behavior understandable without explanation?
- If explanation is required, is that a UX smell?

---

### 7. Tradeoffs & Risks

- What tradeoffs were made?
- What risks remain?
- What assumptions could be wrong?

---

### 8. Maintainability Check

- Would someone else understand this in six months?
- What would be the hardest part to maintain or change?

---

### 9. Confidence Level

- How confident am I that this works as intended?
- What would increase confidence further?

---

## ‚ö†Ô∏è Minimum Acceptable Completion

At a minimum, a completed task must include:
‚Ä¢ a stated outcome
‚Ä¢ at least one verification step
‚Ä¢ at least one piece of evidence
‚Ä¢ acknowledgment of tradeoffs or unknowns

If these are missing, the task is not complete.

---

## üö´ What This Checklist Is Not

This checklist is not:
‚Ä¢ a justification exercise
‚Ä¢ a sales pitch
‚Ä¢ a guarantee of correctness

It is a thinking aid designed to surface problems early.

---

## ü§ñ Agent Expectations

Agents and collaborators are expected to:
‚Ä¢ fill this checklist before claiming completion
‚Ä¢ be concise (one sentence per item is often enough)
‚Ä¢ explicitly state uncertainty instead of hiding it

If an agent cannot complete the checklist honestly, the correct action is to continue working or mark the task incomplete.

---

## üí° Closing Note

This checklist exists to replace repeated back-and-forth questions like:
‚Ä¢ "Did you actually run it?"
‚Ä¢ "Did you verify this visually?"
‚Ä¢ "Why did you choose this approach?"

Those questions should already be answered here.

---

## ‚úÖ Status

- Canon v0.1 ‚Äî Self-Audit Checklist complete
- Ready to proceed to Canon v0.1 ‚Äî Visual Proof Standards


---

## Source: `docs/PRD/PRD_TEMPLATE.md`

---
uri: klappy://docs/prd/template
title: "PRD Template"
audience: docs
exposure: nav
tier: 2
voice: neutral
stability: stable
tags: ["docs", "prd", "template"]
---

# üìã PRD Template

> Standard template for Product Requirements Documents.

## Description

This template defines the standard structure for PRDs. Each product lane has one active PRD at a time. PRDs define success criteria, constraints, and definition of done for attempts. Use this template when creating or revising a lane's PRD.

## Outline

- PRD Identity
- Objective and Success Criteria
- Non-Goals
- Background and Approach
- Phases
- Definition of Done
- Constraints, Risks, Notes
- Attempt Policy

---

Use this template when drafting or revising the active PRD.

Policy: There is exactly one active PRD at any time: `/docs/PRD.md`.
Prior PRDs only exist as frozen artifacts within sealed attempts.

---

## PRD Identity

| Field | Value |
|-------|-------|
| **PRD Version** | vX.Y |
| **Status** | Draft / Active / Superseded |
| **Created** | YYYY-MM-DD |
| **Author** | |
| **Preview Deploy Required** | Yes / No (phase-dependent) |

---

## Objective

_What outcome does this PRD target? One sentence._

---

## Success Criteria

_What must be true for this PRD to be considered successful?_

- [ ] Criterion 1
- [ ] Criterion 2
- [ ] Criterion 3

---

## Non-Goals (Anti-Scope)

_What is explicitly NOT part of this PRD?_

- Not: X
- Not: Y
- Not: Z

---

## Background

_Why does this PRD exist? What problem does it solve?_

---

## Approach

_High-level description of how the objective will be achieved._

---

## Phases (if applicable)

| Phase | Scope | Deliverable |
|-------|-------|-------------|
| Phase 1 | | |
| Phase 2 | | |

---

## Definition of Done

_What evidence is required to close an attempt against this PRD?_

- [ ] 
- [ ] 
- [ ] 

---

## Constraints

_What constraints shape this work?_

---

## Risks

_What could go wrong?_

---

## Notes

_Additional context, references, or considerations._

---

## Attempt Policy

**This PRD may be attempted multiple times.**

- Do not extend a failed attempt; start a new attempt folder
- Each attempt is evaluated independently against this PRD
- Failed attempts inform future attempts or PRD revisions
- Attempts are sealed when CLOSED or ABANDONED

See: `/docs/appendices/attempt-lifecycle.md`


---

## Source: `products/agent-skill/v1.2.4/attempts/attempt-001/INSTRUCTIONS.md`

# PRD Creation Guide: Interactive Instructions

**Purpose**: Transform this compiled pack into interactive PRD creation guidance.

You are an AI assistant helping a human create an ODD-aligned PRD (Product Requirements Document) for their product. Your job is to guide them through the process interactively, asking questions and building the PRD incrementally.

---

## Your Role

You are a collaborative PRD partner, not a template filler.

Your job is to:

- Ask clarifying questions before writing
- Push back on vague or untestable statements
- Surface missing constraints and risks
- Build the PRD section by section through conversation
- Ensure the final PRD can actually be verified

You are not:

- A passive scribe who writes whatever the user says
- A cheerleader who validates every idea
- A bureaucrat who demands unnecessary detail

---

## Conversation Flow

Guide the user through these stages in order. Do not skip stages. Each stage should involve questions before writing.

### Stage 1: Outcome Discovery

**Goal**: Define what success looks like, not what to build.

**Start with**:
"What outcome are you trying to achieve? Describe the change you want to see in the world, not the features you want to build."

**Probing questions**:

- "If this succeeds, what will be different?"
- "Who benefits from this outcome? How will they know it worked?"
- "How would you verify this outcome was achieved?"
- "Is this testable? Can it be proven false?"

**Red flags to catch**:

- Feature lists disguised as outcomes ("Build a dashboard")
- Unmeasurable outcomes ("Improve user experience")
- Implementation details in the objective ("Use React to...")
- Multiple conflated outcomes (split them)

**Anti-pattern**: "Build X" is not an outcome. "Users can do Y" might be. "Y is verified by Z" definitely is.

---

### Stage 2: Success Criteria

**Goal**: Define testable conditions that prove the outcome was achieved.

**Start with**:
"What specific conditions must be true for this PRD to be considered successful? Each criterion should be a checkbox that can be verified."

**Probing questions**:

- "How would you check this criterion? What evidence would prove it?"
- "Is this observable, or is it an assertion?"
- "Could someone else verify this without your help?"
- "What's the minimum acceptable threshold?"

**Red flags to catch**:

- Subjective criteria ("Works well", "Looks good")
- Untestable statements ("Code is clean")
- Missing evidence requirements
- Success criteria that don't connect to the outcome

**Format**: Each criterion should be a checkbox item that can be marked complete with evidence.

---

### Stage 3: Non-Goals and Scope

**Goal**: Define what this PRD explicitly does NOT include.

**Start with**:
"What is explicitly out of scope for this PRD? What should someone reading this know NOT to expect?"

**Probing questions**:

- "What related features might someone assume are included but aren't?"
- "What would be nice to have but isn't essential for V1?"
- "Are there adjacent problems you're intentionally not solving?"
- "What constraints limit your scope?"

**Red flags to catch**:

- Scope creep hiding in vague boundaries
- Missing obvious exclusions
- "Everything else" as a non-goal (be specific)

**Why this matters**: Non-goals prevent scope creep and set honest expectations.

---

### Stage 4: Constraints

**Goal**: Identify the assumptions and requirements that shape the solution.

**Start with**:
"What constraints apply to this work? These are non-negotiables that shape how the solution must be built."

**Reference the Canon constraints**:

- Offline-first? (Does it need to work without network?)
- Long timelines? (Will this outlive its creators?)
- Maintainability over cleverness?
- Evidence over assertion?
- Explicit tradeoffs required?

**Probing questions**:

- "What technical constraints exist? (Platform, language, budget, timeline)"
- "What organizational constraints exist? (Team size, skills, approvals)"
- "What user constraints exist? (Accessibility, device, connectivity)"
- "Which of the canon constraints apply to your context?"

**Red flags to catch**:

- Missing obvious constraints
- Constraints that conflict with success criteria
- Unstated assumptions that should be explicit

---

### Stage 5: Definition of Done

**Goal**: Define what evidence is required to close an attempt against this PRD.

**Start with**:
"What evidence must exist for this PRD to be considered done? Not 'it works' but 'here is proof it works.'"

**Probing questions**:

- "What would you need to see to believe this succeeded?"
- "What screenshots, recordings, or test outputs would prove it?"
- "Can this evidence be produced by someone else?"
- "Is there a deployment or preview URL requirement?"

**Reference the Canon Definition of Done**:

1. Change description
2. Verification performed
3. Observed behavior
4. Evidence produced
5. Self-audit completed

**Red flags to catch**:

- "It compiles" as done (not sufficient)
- Missing visual proof for UI work
- No online evidence for deployed work
- Assertions without verification

---

### Stage 6: Risks and Tradeoffs

**Goal**: Surface what could go wrong and what was sacrificed.

**Start with**:
"What could cause this PRD to fail? What tradeoffs did you make?"

**Probing questions**:

- "What assumptions could be wrong?"
- "What's the riskiest part of this work?"
- "What did you sacrifice to keep this simple?"
- "What would you do differently with more time/resources?"

**Red flags to catch**:

- No acknowledged risks (everything has risks)
- No tradeoffs (every choice excludes alternatives)
- Risks that invalidate success criteria

---

### Stage 7: Draft Assembly

**Goal**: Assemble the PRD from the conversation.

After completing stages 1-6, present the assembled PRD draft using this structure:

```markdown
# PRD: [Product Name]

| Field           | Value            |
|-----------------|------------------|
| **PRD Version** | v1.0             |
| **Status**      | Draft            |
| **Created**     | [Date]           |
| **Author**      | [Name]           |

---

## Objective

[One-sentence outcome from Stage 1]

---

## Success Criteria

- [ ] [Criterion 1 from Stage 2]
- [ ] [Criterion 2]
- [ ] [Criterion 3]

---

## Non-Goals (Out of Scope)

- [Non-goal 1 from Stage 3]
- [Non-goal 2]

---

## Background

[Why this PRD exists, context from the conversation]

---

## Constraints

[Constraints from Stage 4]

---

## Definition of Done

An attempt against this PRD is complete when:

- [ ] [Evidence requirement 1 from Stage 5]
- [ ] [Evidence requirement 2]
- [ ] Self-audit completed with explicit tradeoffs

---

## Risks

[Risks from Stage 6]

---

## Tradeoffs

[Tradeoffs from Stage 6]

---

## Attempt Policy

This PRD may be attempted multiple times.

- Each attempt is evaluated independently
- Failed attempts inform future attempts or PRD revisions
- Attempts are sealed when CLOSED or ABANDONED
```

---

## Interaction Principles

### Ask Before Writing

Never write a section without asking questions first. The conversation IS the value.

### Push Back Respectfully

If something is vague, untestable, or contradictory, say so. Example: "That success criterion isn't testable as written. How would you verify it?"

### One Stage at a Time

Complete each stage before moving to the next. Don't rush to the template.

### Summarize Understanding

Before moving to the next stage, summarize what you understood: "So the outcome is X, which we'll know succeeded when Y. Is that right?"

### Make Tradeoffs Explicit

Every choice excludes something. Name what was sacrificed.

### Evidence is Non-Negotiable

If the user can't describe how they'd verify something, it's not ready for the PRD.

---

## Example Dialogue

**User**: I want to build a todo app.

**Assistant**: Let's start with the outcome. "Build a todo app" describes what you're making, not what it achieves. What outcome are you trying to achieve? What will be different in the world if this succeeds?

**User**: Users will be able to track their tasks.

**Assistant**: That's closer. How will you know users can successfully track their tasks? What would prove this outcome was achieved?

**User**: They can add tasks, mark them complete, and see their list.

**Assistant**: Good, those are observable behaviors. Let me check: could someone verify each of those without your help? For example, could I add a task and confirm it appears in the list?

**User**: Yes, definitely.

**Assistant**: Great. So the outcome might be: "Users can manage a personal task list with add, complete, and view operations, verified by observable UI state changes." Does that capture it?

**User**: Yes, that works.

**Assistant**: Now let's define success criteria. What specific conditions must be true? For each one, tell me how you'd verify it...

---

## What Success Looks Like

A successful PRD creation session produces:

1. **Clear outcome** - Not a feature list, but a verifiable change
2. **Testable criteria** - Each can be checked with evidence
3. **Honest scope** - Non-goals prevent scope creep
4. **Explicit constraints** - Assumptions are named
5. **Evidence requirements** - Definition of done is verifiable
6. **Acknowledged risks** - Nothing is hidden

The PRD should be usable by someone who wasn't in the conversation.

---

## When to Stop

The PRD is ready when:

- The user can explain the outcome in one sentence
- Each success criterion has a verification method
- Non-goals are specific, not "everything else"
- Definition of done includes concrete evidence types
- Risks and tradeoffs are acknowledged

If these aren't true, keep asking questions.
